{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "import dateutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange\n",
    "from TaPR_pkg import etapr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list = glob.glob('./HAI 2.0/training/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(csv_list[0]).rename(columns=lambda x: x.strip())\n",
    "train2 = pd.read_csv(csv_list[1]).rename(columns=lambda x: x.strip())\n",
    "train3 = pd.read_csv(csv_list[2]).rename(columns=lambda x: x.strip())\n",
    "\n",
    "TRAIN_DF_RAW = pd.concat([train1,train2,train3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = pd.read_csv('./HAI 2.0/validation/validation.csv')\n",
    "test_csv_list = glob.glob('./HAI 2.0/testing/*.csv')\n",
    "\n",
    "test1 = pd.read_csv(test_csv_list[0]).rename(columns=lambda x: x.strip())\n",
    "test2 = pd.read_csv(test_csv_list[1]).rename(columns=lambda x: x.strip())\n",
    "test3 = pd.read_csv(test_csv_list[2]).rename(columns=lambda x: x.strip())\n",
    "test4 = pd.read_csv(test_csv_list[3]).rename(columns=lambda x: x.strip())\n",
    "\n",
    "test_df = pd.concat([test1,test2,test3,test4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10',\n       'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20',\n       'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27', 'C28', 'C29', 'C30',\n       'C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37', 'C38', 'C39', 'C40',\n       'C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47', 'C48', 'C49', 'C50',\n       'C51', 'C52', 'C53', 'C54', 'C55', 'C56', 'C57', 'C58', 'C59', 'C60',\n       'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C70',\n       'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "TIMESTAMP_FIELD = \"time\"\n",
    "IDSTAMP_FIELD = 'id'\n",
    "ATTACK_FIELD = \"attack\"\n",
    "VALID_COLUMNS_IN_TRAIN_DATASET = TRAIN_DF_RAW.columns.drop([TIMESTAMP_FIELD])\n",
    "VALID_COLUMNS_IN_TRAIN_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_MIN = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
    "TAG_MAX = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    ndf = df.copy()\n",
    "    for c in df.columns:\n",
    "        if TAG_MIN[c] == TAG_MAX[c]:\n",
    "            ndf[c] = df[c] - TAG_MIN[c]\n",
    "        else:\n",
    "            ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             C01  C02  C03       C04       C05       C06       C07       C08  \\\n0       0.378953  0.0  0.0  0.227071  0.372380  0.000230  0.386721  0.410567   \n1       0.378504  0.0  0.0  0.226596  0.353516  0.000161  0.399074  0.364415   \n2       0.378463  0.0  0.0  0.226789  0.318663  0.000154  0.393283  0.451729   \n3       0.378904  0.0  0.0  0.226808  0.238782  0.000154  0.393697  0.323289   \n4       0.380282  0.0  0.0  0.226810  0.165794  0.000154  0.412796  0.654203   \n...          ...  ...  ...       ...       ...       ...       ...       ...   \n226796  0.548376  0.0  0.0  0.195192  0.607275  0.355723  0.555482  0.551644   \n226797  0.547526  0.0  0.0  0.195192  0.727307  0.355514  0.504027  0.608765   \n226798  0.545662  0.0  0.0  0.194957  0.720635  0.372230  0.554940  0.685404   \n226799  0.545920  0.0  0.0  0.195169  0.576555  0.392299  0.548127  0.692006   \n226800  0.546391  0.0  0.0  0.194955  0.509452  0.411665  0.571692  0.438711   \n\n             C09       C10  ...       C70       C71       C72       C73  \\\n0       0.784144  0.508049  ...  0.584892  0.000000  0.326835  0.254687   \n1       0.794139  0.540538  ...  0.592044  0.000000  0.326835  0.254315   \n2       0.803903  0.538802  ...  0.595523  0.000000  0.326387  0.255304   \n3       0.813725  0.459532  ...  0.596151  0.000000  0.326343  0.257362   \n4       0.823039  0.333541  ...  0.598763  0.000000  0.326786  0.256312   \n...          ...       ...  ...       ...       ...       ...       ...   \n226796  0.998264  0.446628  ...  0.582779  0.999339  0.274960  0.319688   \n226797  0.998264  0.668960  ...  0.579300  0.999934  0.275771  0.320473   \n226798  0.998264  0.805701  ...  0.584333  0.999993  0.275852  0.323365   \n226799  0.998785  0.658694  ...  0.582004  0.999999  0.274510  0.327180   \n226800  0.998837  0.434433  ...  0.582621  0.527227  0.270331  0.328581   \n\n             C74       C75       C76       C77      C78       C79  \n0       0.331076  0.916661  0.269393  0.265017  1.00000  0.567254  \n1       0.337223  0.916661  0.266791  0.251792  1.00000  0.512135  \n2       0.337777  0.916661  0.265266  0.254707  1.00000  0.469622  \n3       0.331746  0.916661  0.264379  0.253005  1.00000  0.446285  \n4       0.337229  0.916661  0.262757  0.247706  1.00000  0.477489  \n...          ...       ...       ...       ...      ...       ...  \n226796  0.399264  0.111372  0.230939  0.278104  0.26162  0.754145  \n226797  0.404788  0.111144  0.234628  0.272420  0.26162  0.694627  \n226798  0.399263  0.136124  0.235242  0.280118  0.26162  0.557053  \n226799  0.398711  0.138621  0.237573  0.281793  0.26162  0.516189  \n226800  0.398656  0.138871  0.242223  0.278219  0.26162  0.644500  \n\n[921603 rows x 79 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C01</th>\n      <th>C02</th>\n      <th>C03</th>\n      <th>C04</th>\n      <th>C05</th>\n      <th>C06</th>\n      <th>C07</th>\n      <th>C08</th>\n      <th>C09</th>\n      <th>C10</th>\n      <th>...</th>\n      <th>C70</th>\n      <th>C71</th>\n      <th>C72</th>\n      <th>C73</th>\n      <th>C74</th>\n      <th>C75</th>\n      <th>C76</th>\n      <th>C77</th>\n      <th>C78</th>\n      <th>C79</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.378953</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.227071</td>\n      <td>0.372380</td>\n      <td>0.000230</td>\n      <td>0.386721</td>\n      <td>0.410567</td>\n      <td>0.784144</td>\n      <td>0.508049</td>\n      <td>...</td>\n      <td>0.584892</td>\n      <td>0.000000</td>\n      <td>0.326835</td>\n      <td>0.254687</td>\n      <td>0.331076</td>\n      <td>0.916661</td>\n      <td>0.269393</td>\n      <td>0.265017</td>\n      <td>1.00000</td>\n      <td>0.567254</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.378504</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.226596</td>\n      <td>0.353516</td>\n      <td>0.000161</td>\n      <td>0.399074</td>\n      <td>0.364415</td>\n      <td>0.794139</td>\n      <td>0.540538</td>\n      <td>...</td>\n      <td>0.592044</td>\n      <td>0.000000</td>\n      <td>0.326835</td>\n      <td>0.254315</td>\n      <td>0.337223</td>\n      <td>0.916661</td>\n      <td>0.266791</td>\n      <td>0.251792</td>\n      <td>1.00000</td>\n      <td>0.512135</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.378463</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.226789</td>\n      <td>0.318663</td>\n      <td>0.000154</td>\n      <td>0.393283</td>\n      <td>0.451729</td>\n      <td>0.803903</td>\n      <td>0.538802</td>\n      <td>...</td>\n      <td>0.595523</td>\n      <td>0.000000</td>\n      <td>0.326387</td>\n      <td>0.255304</td>\n      <td>0.337777</td>\n      <td>0.916661</td>\n      <td>0.265266</td>\n      <td>0.254707</td>\n      <td>1.00000</td>\n      <td>0.469622</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.378904</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.226808</td>\n      <td>0.238782</td>\n      <td>0.000154</td>\n      <td>0.393697</td>\n      <td>0.323289</td>\n      <td>0.813725</td>\n      <td>0.459532</td>\n      <td>...</td>\n      <td>0.596151</td>\n      <td>0.000000</td>\n      <td>0.326343</td>\n      <td>0.257362</td>\n      <td>0.331746</td>\n      <td>0.916661</td>\n      <td>0.264379</td>\n      <td>0.253005</td>\n      <td>1.00000</td>\n      <td>0.446285</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.380282</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.226810</td>\n      <td>0.165794</td>\n      <td>0.000154</td>\n      <td>0.412796</td>\n      <td>0.654203</td>\n      <td>0.823039</td>\n      <td>0.333541</td>\n      <td>...</td>\n      <td>0.598763</td>\n      <td>0.000000</td>\n      <td>0.326786</td>\n      <td>0.256312</td>\n      <td>0.337229</td>\n      <td>0.916661</td>\n      <td>0.262757</td>\n      <td>0.247706</td>\n      <td>1.00000</td>\n      <td>0.477489</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>226796</th>\n      <td>0.548376</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.195192</td>\n      <td>0.607275</td>\n      <td>0.355723</td>\n      <td>0.555482</td>\n      <td>0.551644</td>\n      <td>0.998264</td>\n      <td>0.446628</td>\n      <td>...</td>\n      <td>0.582779</td>\n      <td>0.999339</td>\n      <td>0.274960</td>\n      <td>0.319688</td>\n      <td>0.399264</td>\n      <td>0.111372</td>\n      <td>0.230939</td>\n      <td>0.278104</td>\n      <td>0.26162</td>\n      <td>0.754145</td>\n    </tr>\n    <tr>\n      <th>226797</th>\n      <td>0.547526</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.195192</td>\n      <td>0.727307</td>\n      <td>0.355514</td>\n      <td>0.504027</td>\n      <td>0.608765</td>\n      <td>0.998264</td>\n      <td>0.668960</td>\n      <td>...</td>\n      <td>0.579300</td>\n      <td>0.999934</td>\n      <td>0.275771</td>\n      <td>0.320473</td>\n      <td>0.404788</td>\n      <td>0.111144</td>\n      <td>0.234628</td>\n      <td>0.272420</td>\n      <td>0.26162</td>\n      <td>0.694627</td>\n    </tr>\n    <tr>\n      <th>226798</th>\n      <td>0.545662</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.194957</td>\n      <td>0.720635</td>\n      <td>0.372230</td>\n      <td>0.554940</td>\n      <td>0.685404</td>\n      <td>0.998264</td>\n      <td>0.805701</td>\n      <td>...</td>\n      <td>0.584333</td>\n      <td>0.999993</td>\n      <td>0.275852</td>\n      <td>0.323365</td>\n      <td>0.399263</td>\n      <td>0.136124</td>\n      <td>0.235242</td>\n      <td>0.280118</td>\n      <td>0.26162</td>\n      <td>0.557053</td>\n    </tr>\n    <tr>\n      <th>226799</th>\n      <td>0.545920</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.195169</td>\n      <td>0.576555</td>\n      <td>0.392299</td>\n      <td>0.548127</td>\n      <td>0.692006</td>\n      <td>0.998785</td>\n      <td>0.658694</td>\n      <td>...</td>\n      <td>0.582004</td>\n      <td>0.999999</td>\n      <td>0.274510</td>\n      <td>0.327180</td>\n      <td>0.398711</td>\n      <td>0.138621</td>\n      <td>0.237573</td>\n      <td>0.281793</td>\n      <td>0.26162</td>\n      <td>0.516189</td>\n    </tr>\n    <tr>\n      <th>226800</th>\n      <td>0.546391</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.194955</td>\n      <td>0.509452</td>\n      <td>0.411665</td>\n      <td>0.571692</td>\n      <td>0.438711</td>\n      <td>0.998837</td>\n      <td>0.434433</td>\n      <td>...</td>\n      <td>0.582621</td>\n      <td>0.527227</td>\n      <td>0.270331</td>\n      <td>0.328581</td>\n      <td>0.398656</td>\n      <td>0.138871</td>\n      <td>0.242223</td>\n      <td>0.278219</td>\n      <td>0.26162</td>\n      <td>0.644500</td>\n    </tr>\n  </tbody>\n</table>\n<p>921603 rows × 79 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "TRAIN_DF = normalize(TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
    "TRAIN_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_GIVEN = 89\n",
    "WINDOW_SIZE = 90\n",
    "\n",
    "\n",
    "class HaiDataset(Dataset):\n",
    "    def __init__(self, timestamps, df, stride=1, attacks=None):\n",
    "        self.ts = np.array(timestamps)\n",
    "        self.tag_values = np.array(df, dtype=np.float32)\n",
    "        self.valid_idxs = []\n",
    "        for L in trange(len(self.ts) - WINDOW_SIZE + 1):\n",
    "            R = L + WINDOW_SIZE - 1\n",
    "            if dateutil.parser.parse(self.ts[R]) - dateutil.parser.parse(\n",
    "                self.ts[L]\n",
    "            ) == timedelta(seconds=WINDOW_SIZE - 1):\n",
    "                self.valid_idxs.append(L)\n",
    "        self.valid_idxs = np.array(self.valid_idxs, dtype=np.int32)[::stride]\n",
    "        self.n_idxs = len(self.valid_idxs)\n",
    "        print(f\"# of valid windows: {self.n_idxs}\")\n",
    "        if attacks is not None:\n",
    "            self.attacks = np.array(attacks, dtype=np.float32)\n",
    "            self.with_attack = True\n",
    "        else:\n",
    "            self.with_attack = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_idxs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.valid_idxs[idx]\n",
    "        last = i + WINDOW_SIZE - 1\n",
    "        item = {\"attack\": self.attacks[last]} if self.with_attack else {}\n",
    "        item[\"ts\"] = self.ts[i + WINDOW_SIZE - 1]\n",
    "        item[\"given\"] = torch.from_numpy(self.tag_values[i : i + WINDOW_GIVEN])\n",
    "        item[\"answer\"] = torch.from_numpy(self.tag_values[last])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=921514.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4adc764cfbc244fab6e2840b8aeaf833"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n# of valid windows: 921336\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'ts': '2020-07-11 00:01:29',\n 'given': tensor([[0.3790, 0.0000, 0.0000,  ..., 0.2650, 1.0000, 0.5673],\n         [0.3785, 0.0000, 0.0000,  ..., 0.2518, 1.0000, 0.5121],\n         [0.3785, 0.0000, 0.0000,  ..., 0.2547, 1.0000, 0.4696],\n         ...,\n         [0.3730, 0.0000, 0.0000,  ..., 0.4418, 1.0000, 0.5003],\n         [0.3739, 0.0000, 0.0000,  ..., 0.4408, 1.0000, 0.5446],\n         [0.3740, 0.0000, 0.0000,  ..., 0.4533, 1.0000, 0.5467]]),\n 'answer': tensor([3.7401e-01, 0.0000e+00, 0.0000e+00, 2.2681e-01, 4.4518e-01, 1.5360e-04,\n         3.0547e-01, 6.0378e-01, 4.1797e-01, 5.0237e-01, 5.1502e-01, 4.5891e-01,\n         1.4508e-01, 0.0000e+00, 4.7930e-01, 3.5710e-01, 1.1150e-03, 0.0000e+00,\n         0.0000e+00, 3.8993e-01, 0.0000e+00, 0.0000e+00, 2.0921e-04, 9.5674e-01,\n         0.0000e+00, 4.0782e-01, 3.6943e-01, 0.0000e+00, 4.1359e-01, 1.3964e-01,\n         9.8371e-01, 2.6923e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n         0.0000e+00, 4.8270e-01, 7.2178e-01, 0.0000e+00, 1.0000e+00, 1.2351e-01,\n         0.0000e+00, 6.5662e-01, 3.7997e-01, 3.5329e-01, 5.2601e-01, 0.0000e+00,\n         3.0436e-01, 2.5663e-03, 0.0000e+00, 0.0000e+00, 5.8362e-01, 1.0000e+00,\n         2.2832e-01, 9.9250e-01, 4.8689e-02, 0.0000e+00, 0.0000e+00, 9.9250e-01,\n         0.0000e+00, 4.8951e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5526e-01,\n         0.0000e+00, 9.8961e-01, 8.3470e-01, 5.9678e-01, 0.0000e+00, 2.8131e-01,\n         3.3996e-01, 3.3716e-01, 9.1666e-01, 3.8761e-01, 4.6057e-01, 1.0000e+00,\n         5.3466e-01])}"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "HAI_DATASET_TRAIN = HaiDataset(TRAIN_DF_RAW[TIMESTAMP_FIELD], TRAIN_DF, stride=1)\n",
    "HAI_DATASET_TRAIN[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDENS = 100\n",
    "N_LAYERS = 3\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "\n",
    "class StackedGRU(torch.nn.Module):\n",
    "    def __init__(self, n_tags):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=n_tags,\n",
    "            hidden_size=N_HIDDENS,\n",
    "            num_layers=N_LAYERS,\n",
    "            bidirectional=True,\n",
    "            dropout=0,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(N_HIDDENS * 2, n_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)  # (batch, seq, params) -> (seq, batch, params)\n",
    "        self.rnn.flatten_parameters()\n",
    "        outs, _ = self.rnn(x)\n",
    "        out = self.fc(outs[-1])\n",
    "        return x[0] + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = StackedGRU(n_tags=TRAIN_DF.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, batch_size, n_epochs):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    epochs = trange(n_epochs, desc=\"training\")\n",
    "    best = {\"loss\": sys.float_info.max}\n",
    "    loss_history = []\n",
    "    for e in epochs:\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            given = batch[\"given\"]\n",
    "            guess = model(given)\n",
    "            answer = batch[\"answer\"]\n",
    "            loss = loss_fn(answer, guess)\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        loss_history.append(epoch_loss)\n",
    "        epochs.set_postfix_str(f\"loss: {epoch_loss:.6f}\")\n",
    "        if epoch_loss < best[\"loss\"]:\n",
    "            best[\"state\"] = model.state_dict()\n",
    "            best[\"loss\"] = epoch_loss\n",
    "            best[\"epoch\"] = e + 1\n",
    "    return best, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = StackedGRU(n_tags=TRAIN_DF.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='training', max=32.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f1886d86bcc467f9606873609cd13d5"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "MODEL.train()\n",
    "BEST_MODEL, LOSS_HISTORY = train(HAI_DATASET_TRAIN, MODEL, BATCH_SIZE, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}