{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list =glob.glob('train*.csv')\n",
    "test_list = glob.glob('test*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([pd.read_csv(i)for i in train_list])\n",
    "test = pd.concat([pd.read_csv(i)for i in test_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0         2019-09-11 20:00:00\n1         2019-09-11 20:00:01\n2         2019-09-11 20:00:02\n3         2019-09-11 20:00:03\n4         2019-09-11 20:00:04\n                 ...         \n241195    2019-11-04 14:59:55\n241196    2019-11-04 14:59:56\n241197    2019-11-04 14:59:57\n241198    2019-11-04 14:59:58\n241199    2019-11-04 14:59:59\nName: time, Length: 550800, dtype: object"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = scaler.fit_transform(train.drop(['time'] , axis=1))\n",
    "train_scaled = pd.DataFrame(train_scaled , columns=train.drop('time', axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled['time'] = train['time'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled['time']= pd.to_datetime(train_scaled['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.2.0\n"
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=550792.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "034aedcf173b4bee918f3b01f2f7995b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "309592 ['2019-09-15 09:59:52', '2019-09-15 09:59:53', '2019-09-15 09:59:54', '2019-09-15 09:59:55', '2019-09-15 09:59:56', '2019-09-15 09:59:57', '2019-09-15 09:59:58', '2019-09-15 09:59:59']\n309593 ['2019-09-15 09:59:53', '2019-09-15 09:59:54', '2019-09-15 09:59:55', '2019-09-15 09:59:56', '2019-09-15 09:59:57', '2019-09-15 09:59:58', '2019-09-15 09:59:59', '2019-11-01 20:00:00']\n309594 ['2019-09-15 09:59:54', '2019-09-15 09:59:55', '2019-09-15 09:59:56', '2019-09-15 09:59:57', '2019-09-15 09:59:58', '2019-09-15 09:59:59', '2019-11-01 20:00:00', '2019-11-01 20:00:01']\n309595 ['2019-09-15 09:59:55', '2019-09-15 09:59:56', '2019-09-15 09:59:57', '2019-09-15 09:59:58', '2019-09-15 09:59:59', '2019-11-01 20:00:00', '2019-11-01 20:00:01', '2019-11-01 20:00:02']\n309596 ['2019-09-15 09:59:56', '2019-09-15 09:59:57', '2019-09-15 09:59:58', '2019-09-15 09:59:59', '2019-11-01 20:00:00', '2019-11-01 20:00:01', '2019-11-01 20:00:02', '2019-11-01 20:00:03']\n309597 ['2019-09-15 09:59:57', '2019-09-15 09:59:58', '2019-09-15 09:59:59', '2019-11-01 20:00:00', '2019-11-01 20:00:01', '2019-11-01 20:00:02', '2019-11-01 20:00:03', '2019-11-01 20:00:04']\n309598 ['2019-09-15 09:59:58', '2019-09-15 09:59:59', '2019-11-01 20:00:00', '2019-11-01 20:00:01', '2019-11-01 20:00:02', '2019-11-01 20:00:03', '2019-11-01 20:00:04', '2019-11-01 20:00:05']\n309599 ['2019-09-15 09:59:59', '2019-11-01 20:00:00', '2019-11-01 20:00:01', '2019-11-01 20:00:02', '2019-11-01 20:00:03', '2019-11-01 20:00:04', '2019-11-01 20:00:05', '2019-11-01 20:00:06']\n\n"
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "from datetime import timedelta\n",
    "\n",
    "import dateutil\n",
    "\n",
    "valid_idxs = []\n",
    "timestamp_list = train_scaled['time'].astype(str).tolist()\n",
    "WINDOW_SIZE = 9\n",
    "\n",
    "for L in trange(len(timestamp_list) - WINDOW_SIZE + 1):\n",
    "    R = L + WINDOW_SIZE - 1\n",
    "    if dateutil.parser.parse(timestamp_list[R]) - dateutil.parser.parse(timestamp_list[L]) == timedelta(seconds=WINDOW_SIZE - 1):\n",
    "        valid_idxs.append(L)\n",
    "    else :\n",
    "        print(L, timestamp_list[L:R])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "550791\n550783\n"
    }
   ],
   "source": [
    "print(max(valid_idxs))\n",
    "print(len(valid_idxs)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{309592, 309593, 309594, 309595, 309596, 309597, 309598, 309599}"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "set (list(range(0,550791+1))) - set(valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "309590   2019-09-15 09:59:50\n309591   2019-09-15 09:59:51\n309592   2019-09-15 09:59:52\n309593   2019-09-15 09:59:53\n309594   2019-09-15 09:59:54\n309595   2019-09-15 09:59:55\n309596   2019-09-15 09:59:56\n309597   2019-09-15 09:59:57\n309598   2019-09-15 09:59:58\n309599   2019-09-15 09:59:59\nName: time, dtype: datetime64[ns]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train_scaled[309590:309600]['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = np.array(train_scaled.drop('time',axis=1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in valid_idxs :\n",
    "    item = {} \n",
    "    last = i + WINDOW_SIZE - 1\n",
    "    item[\"given\"] = tag_values[i : last-1]\n",
    "    item[\"answer\"] = tag_values[last]\n",
    "    data_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([9.5041323e-01, 4.8464945e-01, 9.2348807e-02, 9.2435157e-01,\n       8.7563485e-01, 0.0000000e+00, 4.4892412e-03, 7.5154954e-01,\n       0.0000000e+00, 3.0276409e-04, 1.0000000e+00, 9.8799998e-01,\n       4.7067997e-01, 4.7764438e-01, 4.0159157e-01, 6.1166805e-01,\n       1.7618390e-03, 4.8344983e-03, 8.4990239e-01, 8.8116044e-01,\n       5.7252520e-01, 5.9469765e-01, 2.6428846e-01, 1.6577971e-01,\n       1.7335278e-01, 0.0000000e+00, 1.1757903e-01, 2.7608019e-01,\n       1.8668300e-02, 5.9035099e-01, 3.9949328e-01, 5.9121621e-01,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n       7.2916669e-01, 0.0000000e+00, 3.2947403e-01, 4.1381672e-01,\n       5.7635611e-01, 6.9404519e-01, 7.9846829e-01, 6.2500000e-02,\n       7.4573243e-01, 0.0000000e+00, 0.0000000e+00, 3.0887550e-01,\n       5.0649351e-01, 7.9096335e-01, 7.3602962e-01, 0.0000000e+00,\n       6.3928610e-01, 4.8246446e-01, 5.3360903e-01, 5.2754074e-01,\n       9.9067557e-01, 2.8169015e-01, 9.9991953e-01, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "data_list[0]['given'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = np.array(data_list)\n",
    "np.save('train_preprocessed.npy' , data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,name='encoder',latent_dim = 32 , intermediate_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_proj = layers.LSTM(intermediate_dim)\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.lstm_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        \n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    " \n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(layers.Layer) :\n",
    "    def __init__(self, target=56, timestep=10, name='discriminator'):\n",
    "        super(Discriminator , self).__init__()\n",
    "        self.LSTM_discri = layers.LSTM(inputs)\n",
    "        self.dense_result = layers.Dense(1,activation='softmax')\n",
    "    \n",
    "    def(self,inputs) :\n",
    "        x= self.LSTM_discri(inputs)\n",
    "        return self.dense_result(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "  \n",
    "  def __init__(self, original_dim,intermediate_dim=64,name='decoder'):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "    self.lstm_output = layers.LSTM(original_dim)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense_proj(inputs)\n",
    "    return self.lstm_output(x)"
   ]
  }
 ]
}